{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Method 1 :**\n",
    "Disagreement between individual predictors can be interpreted as the information gain associated with adding a datapoint to the training set.\n",
    "\n",
    "    * If a datapoint can be accurately classified by all classifiers, it is easy to classify\n",
    "    * If the point is incorrectly classified by all classifiers, it is an outlier of some sort.\n",
    "    * If a point is incorrectly classified by some, say 50% classifiers, it has some non-trivial learnable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_score import cal_misclassified\n",
    "from dataloader import train_val_test_split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, _, y_train, y_val, _ = train_val_test_split_data(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights associated with each model : {'KNeighbors': 24.958333333333364, 'Random_Forest': 13.770114942528728, 'svm_classifier': 2.224698235840297, 'Gaussian': 2.5300950369588175, 'Decision_Tree': 5.901477832512316, 'Logistic_Reg': 2.8902291917973466}\n",
      "[   2    4    7    8    9   11   12   13   32   34   57   63   80   84\n",
      "   94   95   97  106  118  121  132  136  144  146  152  154  176  180\n",
      "  186  188  195  199  201  208  213  220  223  224  227  229  236  250\n",
      "  251  252  270  295  296  298  307  312  314  318  321  328  330  331\n",
      "  332  337  339  340  341  349  352  358  362  363  368  371  380  390\n",
      "  391  394  397  411  417  424  429  431  432  438  440  446  461  462\n",
      "  468  469  470  471  479  504  509  515  520  521  530  540  543  547\n",
      "  548  550  561  566  580  586  588  602  604  608  611  614  634  635\n",
      "  636  647  652  658  659  660  670  671  675  680  686  688  694  696\n",
      "  710  712  714  717  721  722  723  728  729  737  741  743  749  756\n",
      "  757  763  768  773  780  785  790  791  793  796  798  805  813  814\n",
      "  829  830  833  840  844  848  849  850  855  857  874  882  884  890\n",
      "  891  892  895  896  898  900  908  917  925  926  934  937  940  944\n",
      "  957  958  959  960  961  966  968  977  984  987  988  990  991  998\n",
      " 1000 1002 1003 1007 1013 1017 1026 1029 1037 1043 1060 1061 1062 1065\n",
      " 1070 1079 1085 1090 1095 1098 1105 1113 1116 1119 1126 1130 1137 1144\n",
      " 1146 1153 1156 1160 1161 1165 1173 1175 1182 1191 1192 1195 1197 1205\n",
      " 1208 1213 1219 1222 1226 1230 1239 1243 1246 1250 1253 1260 1261 1266\n",
      " 1268 1273 1275 1285 1289 1290 1293 1296 1297 1298 1305 1306 1307 1308\n",
      " 1309 1310 1314 1328 1330 1331 1332 1342 1345 1350 1351 1355 1358 1369\n",
      " 1372 1378 1387 1395 1396 1408 1410 1412 1418 1419 1423 1427 1429 1430\n",
      " 1432 1436 1441 1450 1455 1457 1458 1460 1462 1464 1474 1495 1497 1498\n",
      " 1501 1505 1506 1512 1515 1516 1519 1529 1533 1536 1538 1542 1543 1549\n",
      " 1550 1559 1560 1569 1576 1583 1584 1585 1590 1595 1597 1598 1601 1603\n",
      " 1605 1610 1613 1616 1617 1629 1630 1633 1637 1640 1642 1643 1649 1658\n",
      " 1662 1668 1674 1676 1677 1683 1687 1688 1692 1693 1694 1698 1706 1708\n",
      " 1724 1728 1737 1739 1740 1747 1751 1763 1765 1766 1772 1776 1780 1790\n",
      " 1791 1802 1805 1809 1810 1812 1816 1818 1819 1822 1825 1830 1842 1848\n",
      " 1850 1856 1865 1870 1874 1878 1882 1886 1893 1899 1901 1903 1908 1909\n",
      " 1917 1923 1926 1928 1931 1944 1946 1948 1952 1955 1962 1965 1979 1984\n",
      " 1987 1988 1992 1999 2002 2003 2007 2013 2020 2026 2032 2048 2051 2054\n",
      " 2066 2068 2072 2074 2077 2087 2095 2098 2104 2105 2107 2110 2127 2132\n",
      " 2134 2135 2143 2154 2155 2161 2169 2173 2176 2178 2195 2199 2204 2207\n",
      " 2212 2217 2235 2236 2237 2242 2253 2254 2264 2265 2266 2270 2272 2273\n",
      " 2276 2277 2281 2286 2298 2302 2317 2328 2332 2333 2334 2338 2340 2341\n",
      " 2345 2358 2361 2366 2367 2368 2371 2374 2378 2383 2386]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditi/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:938: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "err = cal_misclassified(x_train, x_val, y_train, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
